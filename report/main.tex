\documentclass[12pt, a4paper]{article}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{tabularx}
\parindent 0pt
\parskip 5pt
\pagestyle{plain}

\title{\textsc{Consensus}}
\author{}
\date{}

\begin{document}
\maketitle


\section{Motivation}

Achieving agreement among remote processes (where some may be faulty) is a
fundamental problem in distributed computing \cite{fischer1985impossibility}.
This problem is known as \textit{consensus} and lies ``at the core of many
algorithms for distributed data processing, distributed file management, and
fault tolerant distributed applications.'' \cite{fischer1985impossibility}

Due to issues such as node failures and network unreliability, it is difficult
to achieve consistency among nodes in distributed computing or multi-agent
systems.

\section{Formal Definition}

\subsection{Consensus Problem}

The consensus problem is defined with respect to a collection of $N$ processes
which communicate by message passing. Every process $p_{i}$ $(i = 1, \ldots, N)$
begins in the \textit{undecided} state, in which they each propose a single
value $v_{i} \in D$, where $D$ is a set of acceptable values
\cite{coulouris2005distributed}.

The processes then exchange values with each other. During this phase, each
process sets the value of its \textit{decision variable}, $d_i$ $(i = 1, \ldots,
N)$, based on the information obtained from other processes. In doing so, they
enter the \textit{decided} state, in which the decision variable may no longer
change.

A consensus algorithm must satisfy the following conditions for every execution:

\begin{itemize}
  \item \textbf{Termination}: Each non-faulty process $p_i$ must eventually set
    $d_i$.
  \item \textbf{Agreement}: All non-faulty processes in the decided state
    must have the same value. If $p_{i}$ and $p_{j}$ are correct and have
    entered the decided state, then $d_{i} = d_{j}$ $(i, j = 1, \ldots, N)$.
  \item \textbf{Integrity/Validity}: If all non-faulty processes proposed
    the same value, then the decision variable of all non-faulty processes is
    that same value.
\end{itemize}

There are two other problems similar to the consensus problem---the
\textit{Byzantine generals problem} (also known as the \textit{Byzantine
agreement problem}) and the \textit{interactive consistency problem}. Table
\ref{tab:dbtap} provides a comparison between these and the consensus problem.
Despite the differences in requirements, a solution to any one of them can be
transformed to one of the others by reduction \cite{fischer1983consensus}.

\begin{table}[htp]
  \centering
  \begin{tabularx}{\linewidth}{%
    l%
    >{\raggedright\arraybackslash}X%
    >{\raggedright\arraybackslash}X%
    >{\raggedright\arraybackslash}X}
  \toprule
  Condition & Consensus & Byzantine Generals & Interactive Consistency \\
  \midrule
  \textbf{Termination} & Eventually decide on a \textit{value}
    & Eventually decide on a \textit{value}
    & Eventually decide on an \textit{array $A$} \\
  \addlinespace
  \textbf{Agreement} & Agree on the same value
    & Agree on the same value
    & Agree on the same array of values $A[v_{1} \ldots v_{n}]$ \\
  \addlinespace
  \textbf{Integrity}
    & The decided value is based on the propositions of all correct processes
    & If the commander is correct, then all correct processes decide on the
      value proposed by it
    & All correct processes decide on $v_{i}$ as the $i\textsuperscript{th}$
      component of their vector if $p_{i}$ is correct \\
  \bottomrule
\end{tabularx}
  \caption{Comparison of similar problems}
  \label{tab:dbtap}
\end{table}

\subsection{CAP Theorem}

The CAP theorem \cite{brewer2012cap} states that, in a distributed system, it is
impossible for an algorithm to provide more than two of these three properties:

\begin{itemize}
	\item \textbf{Consistency}: All data backups in the distributed system have
    the same value at the same time.
	\item \textbf{Availability}: After some nodes in the cluster fail, the entire
    cluster can still respond to client read and write requests. However, there
    is no guarantee that the data produced is most up-to-date.
	\item \textbf{Partition tolerance}: The system continues to operate despite
    delayed, lost or dropped messages on the network.
\end{itemize}

%% Maybe include in the future.
% \subsection{ACID Theorem}

\subsection{Models of Computation}

The difficulty of solving consensus problem under different system models and
assumptions are varied. Each consensus algorithm has its assumptions about the
problem, and we need to classify and compare based on their assumptions about
the problem.

\subsubsection{Synchronous and Asynchronous Systems}

There are two most popular system models in distributed systems - synchronous
system model and the asynchronous system model. In the synchronous system model,
there is a bound on how long a message takes to be delivered, which means each
message is received within bounded time as long as the sender and recipient
processes are alive. The drift of each process's local clock has a known bound.
Each step in a process takes a time that is lower bounded by a known value and
upper bound by a known value. The bound is a global bound across the entire
state system. E.g., a collection of processors connected by a communication bus.
The asynchronous system model, on the other hand, does not have any bound on
anything. There is no bound on process execution and message transmission
delays. The drift rate of a clock might be arbitrarily fast or arbitrarily slow.
The asynchronous system is a more general model than the synchronous system
model, which also means the consensus problem is more difficult to be solved in
this model. An algorithm work for an asynchronous system will also work for the
asynchronous system. Lots of the very widely used distributed systems adhere to
it. In the synchronous system model, the consensus problem is solvable. However,
the consensus is impossible to solve in the asynchronous system model
\cite{fischer1985impossibility}. Whatever protocol or algorithm is provided,
there is always a worst-case possible execution scenario where some processes
fail, and some messages are delayed, which would prevent the system from
reaching consensus. But actually, this impossibility result derives from a
worst-case scenario that hardly happens. In reality, process scheduling has a
degree of randomness \cite{aguilera2010stumbling}. So, even though it is
theoretically feasible to provide consensus in the asynchronous system model,
through some compromises, practical asynchronous system consensus algorithms can
still be implemented in practice.

\subsubsection{Byzantine and Crash Failures}

According to the types of failures that consensus algorithms can handle, we can
classify them into the following two types:

\begin{itemize}
	\item Byzantine Fault Tolerance (BFT): A BFT Algorithm can handle fault in
    arbitrary ways, including malicious actions of an adversary.
	\item Crash Fault Tolerance (CFT): A CFT Algorithm can handle failure caused
    by processes crash or fail-stop. E.g., Paxos and Raft.
\end{itemize}

Any algorithm that can work for BFT would also work for CFT. That is because
Byzantine failures contain all possible faults in the system model. The relation
of different faults \cite{barborak1993consensus} is shown on Figure
\ref{fig:aofc}.

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.8\textwidth]{img/AOFC.pdf}
  \caption{An ordered fault classification (\textbf{include author here})}
  \label{fig:aofc}
\end{figure}

\subsection{Consensus Algorithms}

Many problems in distributed systems are closely bound up with consensus, which
makes finding effective solutions to consensus problem become significant.

%% Aaron Yau: Maybe I would write this part next week, to give a brief summary
% and comparison of some well-known algorithms instead of paxos or rafe. Or I
% would feel appreciate that someone can continue to write based on the things I
% wrote.}


%% May be needed in the future.
% \section{Impossibility of consensus}
% It has been shown that in an asynchronous system of processes, ``every protocol
% for this problem has the possibility of nontermination, even with only one
% faulty process.'' \cite{fischer1985impossibility}


\section{Relevant problems}


\subsection{Reliable Multicast}

Ensuring that all processes receive updates in the same order.

\subsection{Membership/Failure Detection}

Ensuring that every process has a local record containing every other process.
Failures should be detected and records should be updated.

\subsection{Leader Election}

Deciding on a leader among all processes, with all processes being aware of
who the leader is.

\subsection{Mutual Exclusion}

Ensuring that simultaneous access to a resource does not occur (exclusive
access).


\section{Paxos algorithm}

\subsection{Roles}

A process can take more than one role.

A process is \textit{persistent}---it cannot forget what it has accepted.

  \subsubsection{Proposer}

  \subsubsection{Acceptor}

  \subsubsection{Learner}

\subsection{Protocol}

\subsection{Issues}

  \subsubsection{Contention}

\subsection{Optimisations}


\section{Raft algorithm}
The Raft algorithm was proposed in 2014\cite{conf/usenix/OngaroO14} as an alternative to the (Multi-)Paxos algorithm, which is more
understandable and easier to implement a practical system. The highlight of the Raft algorithm is that it adopts an
engineering thinking --- It simplifies the model of the design according to the requirements in practical applications and
modularizes the process of the algorithm. From the perspective of performance and security, the Raft algorithm is also almost
the same as Paxos.
\subsection{Roles}
  \subsubsection{Follower}
  All nodes are followers when they are started. The followers are completely passive, they can only respond to the incoming
  messages. If the client's message is sent to a follower, the message will be redirected to the leader of this cluster. Followers may
  also be able to become a leader through elections under certain conditions.
  \subsubsection{Leader}
  There can be only one Leader in the entire cluster at the same time. The leader handles all client interactions.
  \subsubsection{Candidate}
  The candidate state is a state between the follower state and the leader state. When a follower becomes a candidate, an election
  will be held. The candidate who wins the election will be the new leader.
\subsection{Protocol}
In the Raft algorithm, time is divided into terms, Each term has a number, and the number increments when a new term starts. Each term starts
with an election. If the election is successful, then the chosen leader will serve out for the rest of the term, which means that
only one leader can be elected in a given term. If the election is not successful, then the candidate starts a new term. Each node
in the cluster maintains the value of the current term number and it should be stored reliably. The function of the term and term number is
to allow the nodes to identify the information that is out of date.
\par
In order to let the followers believe there is an active leader in the cluster, followers expect to receive heartbeat messages from
the leader regularly. If the follower's timeout of heartbeat message elapses, it assumes that the leader crashed and will start a
new election. This timeout is often much longer than the propagation time of the message in the network.
\par
There are only two type of RPCs in the Raft algorithm:
\begin{itemize}
  \item \textbf{RequestVote RPC}: invoked by candidates to gather votes from other nodes.
  \item \textbf{AppendEntries RPC}: invoked by the leader to replicate log entries.
\end{itemize}
  \subsubsection{Leader Election}
  When a node begins an election, the first thing it does is to increment its current term number. The node then converts itself from
  follower state to candidate state. To win the election, the node must receive votes from a majority of nodes in the cluster. The node
  votes for itself, then send RequestVote RPCs to all other nodes and retry this process until:
  \begin{itemize}
    \item It receives votes from majority of nodes in the cluster, which means it wins the election and becomes the leader.
    \item It receives a RPC from a leader, which means one of other candidates wins the election, the current node becomes a follower.
    \item Election timeout elapses, which means no one wins the election. The node will then increment the term number and start a new election
  \end{itemize}
  \par
  To ensure safety, each node can only give out at most one vote in a term. This guarantees that only one candidate can get votes from
  the majority in the same term, thus only one candidate can win the election. Also, to prevent live lock of the election, the election
  timeout of each node is chosen randomly.
  \subsubsection{Log Replication}
  Once a leader is elected, it receives all client's request messages. These messages contain a command to be executed by the replicated
  state machine. The leader creates a log entry for each request from the clients. Apart from commands, the log entry also contains an index
  number in the log and a term number when the log entry was first created. Each log entry is committed if known to be stored in the logs of
  the majority of the nodes.
  \par
  When the leader receives a command from a client, it appends the command with the current term number and index number to its log. Then the leader
  sends AppendEntries RPCs to all followers. Once a new log entry is committed, the leader passes the command to its state machine and
  notifies all followers of this new log entry. When the followers receive these RPCs, they pass the command to their state machine.
  \par
  To ensure consistency of logs, each AppendEntries RPC contains the index and the term of the previous log entry. The followers who receives the RPC will
  reject the request if they do not have a matching log entry. It then is guaranteed that a new entry is only accepted if the logs match in
  their previous entry. So if a given log entry is committed, its all previous log entries are also committed.
  \par
  At the beginning of a new leader's term, the old leaders may have left some log entries that are partially replicated. The way that the Raft
  algorithm used to solve this problem is to force all follower to duplicate its log, which will also force all follower to discard all the
  inconsistent log entries and fill in all missing log entries. The leader maintains a value nextIndex for each follower, which marks the index
  of the next log entry to send to that follower. When AppendEntries consistency check fails, it decrements the nextIndex and retries until the
  follower's log is repaired.
  \subsubsection{Safety}
  If a leader has decided that a log entry is committed, then that entry should be present in the logs of all future leaders. To guarantee this,
  the algorithm picks the candidate to win the election such that it has the log that is most likely to have all the entries that have been
  committed. So the candidates must include log info in RequestVote RPCs including index and term of the last log entry. And the voting servers
  have to deny if its log is more complete.
  \par
  Another safety issue is that if a leader crashes while replicating logs, the new leader will not know whether the last log entry of the
  previous leader has been committed or not. To solve this problem, a new commitment rule must apply --- For a leader to decide an entry
  is committed, in addition to the previous rule, at least one new entry from leader's term must also be stored on the majority of servers.
\subsection{Issues}

\subsection{Optimisations}


\section{Comparative analysis}


\section{Application}


\bibliographystyle{ieeetr}
\bibliography{main}

\end{document}
